{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# model = \"gpt-3.5-turbo-0613\"\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"page_title\": {\"type\": \"string\", \"description\": \"The title of the webpage.\"},\n",
    "        \"meta_description\": {\"type\": \"string\", \"description\": \"Meta description of the webpage.\"},\n",
    "        \"author\": {\"type\": \"string\", \"description\": \"The author or source of the content.\"},\n",
    "        \"publish_date\": {\"type\": \"string\", \"description\": \"The publish date if available.\"},\n",
    "        \"main_content\": {\"type\": \"string\", \"description\": \"The main text content of the webpage.\"},\n",
    "        \"sections\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"Different sections or paragraphs of the content.\",\n",
    "        },\n",
    "        \"links\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"List of URLs found in the page content.\",\n",
    "        },\n",
    "        \"images\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"List of image URLs found in the page.\",\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"page_title\", \"main_content\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from playwright.async_api import TimeoutError as PlaywrightTimeoutError\n",
    "\n",
    "# Load API keys\n",
    "dotenv.load_dotenv()\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "class RobustChromiumLoader(AsyncChromiumLoader):\n",
    "    \"\"\"\n",
    "    Ultimate Fix: Handles:\n",
    "    - Late JavaScript navigation\n",
    "    - Unwanted dynamic changes\n",
    "    - Auto-redirects\n",
    "    - Pop-ups & unnecessary elements\n",
    "    \"\"\"\n",
    "    async def _fetch(self, page, url):\n",
    "        try:\n",
    "            # Set User-Agent to mimic a real browser\n",
    "            await page.set_extra_http_headers({\"User-Agent\": USER_AGENT})\n",
    "\n",
    "            # Block auto-navigation & unnecessary assets (ads, images, tracking scripts)\n",
    "            async def block_unwanted(route):\n",
    "                if route.request.resource_type in [\"image\", \"font\", \"stylesheet\", \"media\"]:\n",
    "                    await route.abort()\n",
    "                else:\n",
    "                    await route.continue_()\n",
    "            await page.route(\"**/*\", block_unwanted)\n",
    "\n",
    "            # **Prevent Unwanted Redirections**\n",
    "            async def intercept_navigation(request):\n",
    "                if request.is_navigation_request():\n",
    "                    await request.abort()  # Block redirects\n",
    "            await page.route(\"**/*\", intercept_navigation)\n",
    "\n",
    "            # Go to the page, ensuring full JavaScript execution\n",
    "            await page.goto(url, wait_until=\"domcontentloaded\", timeout=25000)\n",
    "            await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "            # Ensure key elements are present before extracting\n",
    "            await page.wait_for_selector(\"body\", timeout=10000)\n",
    "\n",
    "            # **Handle Delayed Content**: Wait for final JavaScript updates\n",
    "            await asyncio.sleep(2)  # Ensures async-loaded content is fully rendered\n",
    "\n",
    "            # **Safe Extraction**: Avoids page.content() error\n",
    "            html_handle = await page.evaluate_handle(\"document.documentElement.outerHTML\")\n",
    "            html_content = await html_handle.json_value()  # Convert to string\n",
    "            \n",
    "            return html_content\n",
    "\n",
    "        except PlaywrightTimeoutError:\n",
    "            print(f\"‚è≥ Timeout while loading {url}. Retrying...\")\n",
    "            return await self._retry_fetch(page, url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def _retry_fetch(self, page, url, retries=3):\n",
    "        \"\"\"Retry fetching the page with incremental wait times.\"\"\"\n",
    "        wait_times = [3, 5, 8]  # Gradual increase in delay before retrying\n",
    "        for attempt, wait_time in enumerate(wait_times, start=1):\n",
    "            try:\n",
    "                print(f\"üîÑ Retrying ({attempt}/{retries}) for {url}, waiting {wait_time}s...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "                await page.goto(url, wait_until=\"domcontentloaded\", timeout=25000)\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "                await page.wait_for_selector(\"body\", timeout=10000)\n",
    "                html_handle = await page.evaluate_handle(\"document.documentElement.outerHTML\")\n",
    "                return await html_handle.json_value()\n",
    "            except PlaywrightTimeoutError:\n",
    "                continue\n",
    "        print(f\"‚ùå Final retry failed for {url}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "\n",
    "async def scrape_and_extract(urls):\n",
    "    \"\"\"\n",
    "    Uses AsyncChromiumLoader to load the webpage asynchronously and extract clean text.\n",
    "    \"\"\"\n",
    "    loader = RobustChromiumLoader(urls=urls)\n",
    "    docs = await loader.aload()\n",
    "\n",
    "    transformer = Html2TextTransformer()\n",
    "    text_docs = transformer.transform_documents(docs)\n",
    "\n",
    "    if not text_docs:\n",
    "        return None\n",
    "    \n",
    "    extracted_data = []\n",
    "    for doc in text_docs:\n",
    "        content = doc.page_content\n",
    "        extracted_info = create_extraction_chain(schema=schema, llm=llm).run(content)\n",
    "        extracted_data.append(extracted_info)\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_documents_to_files(documents, directory=\"./output/\"):\n",
    "    \"\"\"\n",
    "    Saves each extracted document to a .txt file.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(directory, f\"file.json\") \n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(documents, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    urls = [\n",
    "        \"https://www.vensure.com/\",\n",
    "        \"https://www.vensure.com/about-us/overview/\"\n",
    "    ]\n",
    "\n",
    "    extracted_data = await scrape_and_extract(urls)\n",
    "\n",
    "    if extracted_data:\n",
    "        save_documents_to_files(extracted_data)\n",
    "    else:\n",
    "        print(\"‚ùå No data extracted!\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
